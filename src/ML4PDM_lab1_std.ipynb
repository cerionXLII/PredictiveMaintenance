{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "- Library Installation\n",
        "- Dataset - histograms of wet tank air pressures from buses\n",
        "- Anomaly Detection with COSMO method\n",
        "- Reviewing anomaly scores with repair events"
      ],
      "metadata": {
        "id": "HFPtcLXg4DQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing required libraries\n",
        "\n",
        "Let's start by installing required libraries."
      ],
      "metadata": {
        "id": "aR-KPb_jEk6b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukzSNWkV3yKS"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "import os, math\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "metadata": {
        "id": "uwRdgl4QE6K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "We start by loading the dataset."
      ],
      "metadata": {
        "id": "hpyHI_ZQEzD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "file_dataset = \"1cNE-d8-43XY6s7eIOSHZSq1P2gbn2d34\"\n",
        "#https://drive.google.com/file/d/1cNE-d8-43XY6s7eIOSHZSq1P2gbn2d34/view?usp=sharing\n",
        "downloaded = drive.CreateFile({'id': file_dataset})\n",
        "downloaded.GetContentFile('lab1_example_dataset.csv')\n",
        "print(os.listdir())\n",
        "df_hist=pd.read_csv('lab1_example_dataset.csv')"
      ],
      "metadata": {
        "id": "vnc5GXyGE491"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hist.head(5)"
      ],
      "metadata": {
        "id": "ViFW2mcbImaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hist.describe()"
      ],
      "metadata": {
        "id": "-GM3i4X6Iurt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hist[\"t\"] = pd.to_datetime(df_hist[\"t\"])"
      ],
      "metadata": {
        "id": "FS9gzJipKl5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection with COSMO method\n",
        "\n",
        "Implementing the COSMO method according to the pseudocode, and material in slides.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uq_iZ2joJVLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "\n",
        "random_state = 42\n",
        "nn_samples_reference_group = 50\n",
        "histogram_param = [xx for xx in df_hist.columns if \"bin\" in xx]\n",
        "\n",
        "df_hist['date_week'] = df_hist['t'].dt.to_period('W')\n",
        "week_lst = list(df_hist['date_week'].unique())\n",
        "T_u = 4 # 4 weeks\n",
        "\n",
        "df_hist[\"z_score\"] = np.nan\n",
        "df_hist[\"anomaly_score\"] = np.nan\n",
        "\n",
        "# Selected metric - Hellinger distance\n",
        "def hellinger_dist(u, v):\n",
        "    a = 0.0\n",
        "    for v1,v2 in zip(np.array(u).flat,np.array(v).flat):\n",
        "        a += (math.sqrt(v1)-math.sqrt(v2))**2\n",
        "    return math.sqrt(a/2)\n",
        "\n",
        "# Computing distance matrix\n",
        "def compute_distance_matrix(samples):\n",
        "    def symmetrize(a):\n",
        "        return a + a.T - np.diag(a.diagonal())\n",
        "    \n",
        "    dist_matrix = np.zeros((len(samples), len(samples)))\n",
        "    \n",
        "    for ir in range(len(samples)):\n",
        "        for ic in range(ir, len(samples)):\n",
        "            dist_matrix[ir, ic] = hellinger_dist(samples[ir], samples[ic])\n",
        "\n",
        "    return symmetrize(dist_matrix)\n",
        "\n",
        "# Acquire the most central pattern\n",
        "def get_mcp(samples):\n",
        "    dist_matrix = compute_distance_matrix(samples)\n",
        "    mcp_idx = dist_matrix.sum(axis=1).argmin()\n",
        "    return samples[mcp_idx], dist_matrix[mcp_idx]\n"
      ],
      "metadata": {
        "id": "vR8LUyZVJUYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iweek, week in enumerate(week_lst):\n",
        "\n",
        "    # acquire the reference group\n",
        "\n",
        "    # compute pairwise distance matrix\n",
        "\n",
        "    # acquire most central pattern\n",
        "\n",
        "    # compute z-score"
      ],
      "metadata": {
        "id": "r3bM_DLkNQvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normcdf(x, mu, sigma):\n",
        "    t = x-mu\n",
        "    y = 0.5*math.erfc(-t/(sigma*math.sqrt(2.0)))\n",
        "    if y>1.0:\n",
        "        y = 1.0\n",
        "    return y\n",
        "\n",
        "def getArithmeticP_val(gMu, n):\n",
        "    if n == 0:\n",
        "        return float('NaN')\n",
        "    amu = 0.5 # mean\n",
        "    asigma = np.sqrt(1.0/(12.0*n)) # Standard deviation for the mean\n",
        "    return normcdf(gMu, amu, asigma)\n",
        "\n",
        "# for each unit, compute the anomaly score based on the z-scores\n",
        "for i_vid, vid in enumerate(df_hist[\"id\"].unique()):\n",
        "    \n",
        "    veh_readings = df_hist[df_hist[\"id\"]==vid].copy().sort_values(by=['t'])\n",
        "    \n",
        "    for iweek, week in enumerate(week_lst):\n",
        "\n",
        "        # compute mean values of the z-score over the period of T_u\n",
        "\n",
        "        # compute arithmatic p-value"
      ],
      "metadata": {
        "id": "B358DrroN3il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly scores and repair events\n",
        "\n",
        "Let's review anomaly scores and a few repair events."
      ],
      "metadata": {
        "id": "b-oML1MNK2DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events = {\n",
        "    0: [\"2012-07-02\", 1],\n",
        "    4: [\"2012-06-04\", 0],\n",
        "    6: [\"2012-09-17\", 1],\n",
        "    12: [\"2012-03-23\", 0],\n",
        "    13: [\"2012-10-18\", 1]\n",
        "}\n",
        "\n",
        "plt.clf()\n",
        "\n",
        "for i_vid, vid in enumerate(df_hist[\"id\"].unique()):\n",
        "    \n",
        "    veh_readings = df_hist[df_hist[\"id\"]==vid].copy().sort_values(by=['t'])\n",
        "    \n",
        "    plt.figure(figsize=(13, 2))\n",
        "    plt.plot(veh_readings[\"t\"], veh_readings[\"anomaly_score\"])\n",
        "    plt.ylabel(vid)\n",
        "    plt.xlim(df_hist[\"t\"].min(), df_hist[\"t\"].max())\n",
        "    plt.ylim([0, 15])\n",
        "    \n",
        "    print(vid, events)\n",
        "    if vid in events:\n",
        "        if events[vid][1]:\n",
        "            plt.axvline(x=pd.to_datetime(events[vid][0]), c=\"r\")\n",
        "        else:\n",
        "            plt.axvline(x=pd.to_datetime(events[vid][0]), c=\"y\")"
      ],
      "metadata": {
        "id": "90suzjCEK2U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "\n",
        "for i_vid, vid in enumerate(df_hist[\"id\"].unique()):\n",
        "\n",
        "    veh_readings = df_hist[df_hist[\"id\"]==vid].copy().sort_values(by=['t'])\n",
        "    \n",
        "    plt.figure(figsize=(13, 2))\n",
        "    plt.scatter(veh_readings[\"t\"], veh_readings[\"z_score\"], s=5, marker=\"o\")\n",
        "    plt.xlim(df_hist[\"t\"].min(), df_hist[\"t\"].max())\n",
        "    plt.ylabel(vid)"
      ],
      "metadata": {
        "id": "rUQJd6u9Oo-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference\n",
        "\n",
        "- Fan, Y., Nowaczyk, S., & Rögnvaldsson, T. (2015). Evaluation of self-organized approach for predicting compressor faults in a city bus fleet. Procedia Computer Science, 53, 447-456.\n",
        "\n",
        "- Fan, Yuantao, Sławomir Nowaczyk, Thorsteinn Rögnvaldsson, and Eric Aislan Antonelo. \"Predicting air compressor failures with echo state networks.\" PHM Society European Conference. Vol. 3. No. 1. 2016.\n"
      ],
      "metadata": {
        "id": "opz2Cy2cRYb6"
      }
    }
  ]
}